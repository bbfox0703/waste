# -*- coding: utf-8 -*-
"""
NLLB ç¿»è­¯ä¼ºæœå™¨ï¼ˆå®Œæ•´æ›¿æ›ç‰ˆï¼‰
- ä»¥ NLLB-200 ç‚ºæ ¸å¿ƒï¼šæ”¯æ´ enâ†”zh(Hant/Hans)ã€jaâ†”zhã€jaâ†”en
- 16GB VRAM å»ºè­°ï¼šé è¨­ 1.3B + FP16ï¼›æƒ³æ›´å¥½å¯æ”¹ 3.3B + 8bit é‡åŒ–
- ä¿ç•™ OpenCC(å°ç£åŒ–) èˆ‡è‡ªè¨‚è©å½™è£œä¸ã€æ™ºæ…§æ›è¡Œ
"""

from flask import Flask, request, Response
from flask_cors import CORS
import urllib.parse
import warnings
import re
import torch
import opencc
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

warnings.filterwarnings("ignore", category=FutureWarning)

# =========================
# å¯èª¿åƒæ•¸ï¼ˆä¾ä½ çš„æ©Ÿå™¨èˆ‡éœ€æ±‚èª¿æ•´ï¼‰
# =========================
# æ¨¡å‹ï¼š1.3Bï¼ˆç©©ï¼‰æˆ– 3.3Bï¼ˆå“è³ªæ›´å¥½ï¼Œå»ºè­°é… 8bitï¼‰
NLLB_MODEL = "facebook/nllb-200-1.3B"     # æ”¹æˆ "facebook/nllb-200-3.3B" å¯å‡ç´š
USE_8BIT = False                           # è‹¥ç”¨ 3.3B å»ºè­° Trueï¼ˆéœ€ pip install bitsandbytesï¼‰
USE_4BIT = False                           # æˆ–è€… 4bitï¼›èˆ‡ 8bit äºŒé¸ä¸€
# ç”Ÿæˆåƒæ•¸ï¼ˆå“è³ªè¦ç´ ï¼‰
GEN_NUM_BEAMS = 8
GEN_NO_REPEAT_NGRAM = 3
GEN_LENGTH_PENALTY = 1.05
MAX_SRC_LEN = 1024
MAX_NEW_TOKENS = 512

# æœå‹™è¨­å®š
DEFAULT_WRAP = True        # é è¨­è‡ªå‹•æ›è¡Œ
DEFAULT_MAX_CHARS = 1000
DEFAULT_SPLIT_THRESHOLD = 2400

# =========================
# Flask
# =========================
app = Flask(__name__)
CORS(app)

# =========================
# OpenCC & è‡ªè¨‚è©å½™
# =========================
opencc_s2t = opencc.OpenCC('s2twp')  # ç°¡â†’ç¹ï¼ˆå°ç£ç”¨èªï¼‰
opencc_t2s = opencc.OpenCC('tw2s')   # ç¹â†’ç°¡

custom_zhcn_to_zh_tw = {
    "å…‰é©…": "å…‰ç¢Ÿæ©Ÿ",
    "è»Ÿé©…": "è»Ÿç¢Ÿæ©Ÿ",
    "ç¸½ç·š": "åŒ¯æµæ’",
    "å½ä»£ç¢¼": "è™›æ“¬ç¢¼",
    "æ•¸æ“šåº«": "è³‡æ–™åº«",
    "æ“ä½œç³»çµ±": "ä½œæ¥­ç³»çµ±",
    "å¯åŸ·è¡Œæ–‡ä»¶": "åŸ·è¡Œæª”",
    "ç­†è¨˜æœ¬é›»è…¦": "ç­†è¨˜å‹é›»è…¦",
    "äºŒæ¥µç®¡": "äºŒæ¥µé«”",
    "ä¸‰æ¥µç®¡": "ä¸‰æ¥µé«”",
    "æœå‹™å™¨": "ä¼ºæœå™¨",
    "å±€åŸŸç¶²": "å€åŸŸç¶²è·¯",
    "é«˜é€Ÿç·©å­˜": "å¿«å–è¨˜æ†¶é«”",
    "é¼ æ¨™": "æ»‘é¼ ",
    "è»Ÿä»¶": "è»Ÿé«”",
    "ç¡¬ä»¶": "ç¡¬é«”",
    "æ–‡ä»¶": "æª”æ¡ˆ",
    "è¨­ç½®": "è¨­å®š",
    "ç²˜è²¼": "è²¼ä¸Š",
    "æ¸¸æˆ²": "éŠæˆ²",
    "ä¸»é ": "é¦–é ",
    "åœ–åƒ": "å½±åƒ",
    "åœ–æ¨™": "åœ–ç¤º",
    "å­—ä½“": "å­—å‹",
    "é€£ç¶²": "é€£ç·š",
    "ç¶²çµ¡": "ç¶±è·¯",
    "ç™»é™¸": "ç™»å…¥",
    "æ³¨å†Œ": "è¨»å†Š",
    "è³¬è™Ÿ": "å¸³è™Ÿ",
    "è³¬æˆ¶": "å¸³æˆ¶",
    "éƒµç®±": "é›»å­éƒµä»¶",
    "æ•¸æ“š": "è³‡æ–™",
    "å±å¹•": "è¢å¹•",
    "æ”å½±é ­": "æ”å½±æ©Ÿ",
    "çª—å£": "è¦–çª—",
    "æ‡‰ç”¨ç¨‹åº": "æ‡‰ç”¨ç¨‹å¼",
    "æ§åˆ¶é¢æ¿": "æ§åˆ¶å°",
    "å¿«æ·éµ": "å¿«é€Ÿéµ",
    "ç•Œé¢": "ä»‹é¢",
    "ä¿å­˜": "å„²å­˜",
    "åŠ è¼‰": "è¼‰å…¥",
    "æ‰“å°": "å°è¡¨",
    "æ‰“å°æ©Ÿ": "å°è¡¨æ©Ÿ",
    "ç¼ºçœ": "é è¨­",
    "æ‰‹æ¸¸": "æ‰‹éŠ",
    "ç¡¬ç›¤": "ç¡¬ç¢Ÿ",
    "å…§å­˜": "è¨˜æ†¶é«”",
    "é¢åŒ…": "éºµåŒ…",
    "è¦–é »": "å½±ç‰‡",
    "å…‰ç›¤": "å…‰ç¢Ÿ",
    "ç›¤ç‰‡": "ç¢Ÿç‰‡",
    "ç¡…ç‰‡": "çŸ½ç‰‡",
    "ç¡…è°·": "çŸ½è°·",
    "ç£ç›¤": "ç£ç¢Ÿ",
    "ç£é“": "ç£è»Œ",
    "Uç›¤": "éš¨èº«ç¢Ÿ",
    "ä¸²è¡Œ": "ä¸²åˆ—",
    "å‰ç¶´": "é¦–ç¢¼",
    "å¾Œç¶´": "å°¾ç¢¼",
    "ç­‰é›¢å­": "é›»æ¼¿",
    "æ–¹ä¾¿é¢": "æ³¡éºµ",
    "åœŸè±†": "é¦¬éˆ´è–¯",
    "æœ´ç´ ": "æ¨¸ç´ ",
    "å¯¬å¸¶": "å¯¬é »",
    "å¸¶å¯¬": "é »å¯¬",
    "æ¨¡å¡Š": "æ¨¡çµ„",
    "çŸ­ä¿¡": "ç°¡è¨Š",
    "å†…å­˜": "è¨˜æ†¶é«”",
    "å…‰æ¨™": "æ¸¸æ¨™"
}

def patch_custom_terms(text: str) -> str:
    for k, v in custom_zhcn_to_zh_tw.items():
        text = text.replace(k, v)
    return text

def remove_extra_spaces_for_japanese(text):
    # å°‡æ—¥æ–‡ä¸­çš„ã€Œå­— å­— å­—ã€è®Šæˆã€Œå­—å­—å­—ã€
    return re.sub(r'(?<=[\u3040-\u30FF\u4E00-\u9FFF])\s+(?=[\u3040-\u30FF\u4E00-\u9FFF])', '', text)

def smart_linebreak(text, max_chars=35, word_split_threshold=2400):
    import unicodedata
    def count_length(s):
        count = 0
        for c in s:
            if unicodedata.east_asian_width(c) in ('F', 'W', 'A'):
                count += 1
            else:
                count += 0.5
        return count
    lines = text.split("<eol>")
    result_lines = []
    for line in lines:
        line = line.strip()
        total_length = count_length(line)
        if total_length <= word_split_threshold:
            result_lines.append(line)
        else:
            result_lines.extend(line.split())
    return "\n".join(result_lines)

# =========================
# NLLB è¼‰å…¥
# =========================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

if USE_8BIT or USE_4BIT:
    from transformers import BitsAndBytesConfig
    bnb_config = BitsAndBytesConfig(
        load_in_8bit=USE_8BIT,
        load_in_4bit=USE_4BIT
    )
    tokenizer_nllb = AutoTokenizer.from_pretrained(NLLB_MODEL)
    model_nllb = AutoModelForSeq2SeqLM.from_pretrained(
        NLLB_MODEL,
        quantization_config=bnb_config,
        device_map="auto"
    )
else:
    tokenizer_nllb = AutoTokenizer.from_pretrained(NLLB_MODEL)
    model_nllb = AutoModelForSeq2SeqLM.from_pretrained(
        NLLB_MODEL,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        device_map="auto"
    )

# NLLB èªè¨€ç¢¼
LANG = dict(
    EN="eng_Latn",
    ZH_HANT="zho_Hant",
    ZH_HANS="zho_Hans",
    JA="jpn_Jpan",
)

def nllb_translate(text: str, src_lang: str, tgt_lang: str,
                   max_src_len=MAX_SRC_LEN, max_new_tokens=MAX_NEW_TOKENS,
                   num_beams=GEN_NUM_BEAMS, no_repeat_ngram_size=GEN_NO_REPEAT_NGRAM,
                   length_penalty=GEN_LENGTH_PENALTY):
    tokenizer_nllb.src_lang = src_lang
    inputs = tokenizer_nllb(
        text, return_tensors="pt", padding=True, truncation=True, max_length=max_src_len
    ).to(device)
    gen = model_nllb.generate(
        **inputs,
        forced_bos_token_id=tokenizer_nllb.convert_tokens_to_ids(tgt_lang),
        num_beams=num_beams,
        no_repeat_ngram_size=no_repeat_ngram_size,
        length_penalty=length_penalty,
        max_new_tokens=max_new_tokens,
        early_stopping=True
    )
    return tokenizer_nllb.batch_decode(gen, skip_special_tokens=True)[0]

def nllb_translate_by_lines(text: str, src_lang: str, tgt_lang: str) -> str:
    """ä¾ <eol> é€å¥ç¿»ï¼Œé¿å…è¶…é•·æˆªæ–·ï¼Œæ•´é«”å“è³ªæ›´ç©©ã€‚"""
    parts = text.split("<eol>")
    outs = []
    for seg in parts:
        seg = seg.strip()
        if not seg:
            outs.append("")  # ä¿ç•™ç©ºè¡Œ
            continue
        try:
            outs.append(nllb_translate(seg, src_lang, tgt_lang))
        except Exception as _:
            # å¦‚æœå–®å¥ä»å¤±æ•—ï¼Œå˜—è©¦ç¸®çŸ­é•·åº¦
            outs.append(nllb_translate(seg[:4000], src_lang, tgt_lang))
    return "<eol>".join(outs)

# =========================
# è·¯ç”±
# =========================
@app.route("/ping")
def ping():
    return "pong"

@app.route("/translate", methods=["GET", "POST"])
def translate():
    wrap = DEFAULT_WRAP
    if request.method == "GET":
        text = request.args.get("text", "")
        from_lang = request.args.get("from", "zh-cn")
        to_lang = request.args.get("to", "zh")
        wrap = request.args.get("wrap", "true").lower() != "false"
        max_chars = int(request.args.get("max_chars", DEFAULT_MAX_CHARS))
        word_split_threshold = int(request.args.get("word_split_threshold", DEFAULT_SPLIT_THRESHOLD))
    else:
        data = request.get_json(force=True)
        text = data.get("text", "")
        from_lang = data.get("from", "zh-cn")
        to_lang = data.get("to", "zh")
        wrap = str(data.get("wrap", "true")).lower() != "false"
        max_chars = int(data.get("max_chars", DEFAULT_MAX_CHARS))
        word_split_threshold = int(data.get("word_split_threshold", DEFAULT_SPLIT_THRESHOLD))

    # é è¨­ zh ç‚º zh-tw
    if from_lang == "zh":
        from_lang = "zh-tw"
    if to_lang == "zh":
        to_lang = "zh-tw"

    print(f"ğŸ“¥ ç¿»è­¯è«‹æ±‚: from={from_lang}, to={to_lang}")

    # URL decode & ä»¥ <eol> è¡¨ç¤ºæ›è¡Œ
    text = urllib.parse.unquote(text)
    text = re.sub(r'(\r\n|\r|\n|%0A|%0D|%0D%0A)', '<eol>', text)

    try:
        # ---------------------------
        # èªè¨€å°æ‡‰ï¼ˆä¾†æº/ç›®æ¨™ â†’ NLLB èªè¨€ç¢¼ï¼‰
        # ---------------------------
        def zh_target_code():
            return LANG["ZH_HANT"] if to_lang in ["zh-tw", "zh"] else LANG["ZH_HANS"]

        def zh_source_code():
            # ä¾†æºç‚º zh-tw/zh â†’ ç•¶ä½œç¹ï¼›ä¾†æºç‚º zh-cn â†’ ç•¶ä½œç°¡
            return LANG["ZH_HANT"] if from_lang in ["zh-tw", "zh"] else LANG["ZH_HANS"]

        # ================= en â†’ zh =================
        if from_lang == "en" and to_lang.startswith("zh"):
            final_text = nllb_translate_by_lines(text, LANG["EN"], zh_target_code())

        # ================= zh â†’ en =================
        elif from_lang in ["zh-tw", "zh", "zh-cn"] and to_lang == "en":
            final_text = nllb_translate_by_lines(text, zh_source_code(), LANG["EN"])

        # ================= ja â†’ zh =================
        elif from_lang == "ja" and to_lang.startswith("zh"):
            final_text = nllb_translate_by_lines(text, LANG["JA"], zh_target_code())

        # ================= zh â†’ ja =================
        elif from_lang in ["zh-tw", "zh", "zh-cn"] and to_lang == "ja":
            final_text = nllb_translate_by_lines(text, zh_source_code(), LANG["JA"])
            final_text = remove_extra_spaces_for_japanese(final_text)

        # ================= ja â†’ en =================
        elif from_lang == "ja" and to_lang == "en":
            final_text = nllb_translate_by_lines(text, LANG["JA"], LANG["EN"])

        # ================= en â†’ ja =================
        elif from_lang == "en" and to_lang == "ja":
            final_text = nllb_translate_by_lines(text, LANG["EN"], LANG["JA"])
            final_text = remove_extra_spaces_for_japanese(final_text)

        # ================= zh äº’è½‰ï¼ˆOpenCCï¼‰=================
        elif from_lang in ["zh-tw", "zh"] and to_lang == "zh-cn":
            final_text = opencc_t2s.convert(text)
        elif from_lang == "zh-cn" and to_lang in ["zh-tw", "zh"]:
            final_text = text  # å…ˆäº¤çµ¦å¾Œè™•ç†åšå°ç£åŒ–
        else:
            return Response("[error] æš«ä¸æ”¯æ´æ­¤èªè¨€å°", content_type="text/plain; charset=utf-8")

        # ---------------------------
        # å¾Œè™•ç†ï¼ˆå°ç£åŒ– + è‡ªè¨‚è©å½™ / æ—¥æ–‡ç©ºç™½ä¿®æ­£ / æ™ºæ…§æ›è¡Œï¼‰
        # ---------------------------
        if to_lang == "zh-tw":
            # å³ä½¿ NLLB ç›®æ¨™æ˜¯ zho_Hantï¼Œä»ä»¥ OpenCC åšå°ç£åŒ–è©å½™å„ªåŒ–
            final_text = opencc_s2t.convert(final_text)
            final_text = patch_custom_terms(final_text)

        if to_lang == "ja":
            final_text = remove_extra_spaces_for_japanese(final_text)

        final_text = final_text.replace("<eol>", "\n")
        if wrap:
            final_text = smart_linebreak(final_text, max_chars=max_chars, word_split_threshold=word_split_threshold)

        print(f"âœ… ç¿»è­¯çµæœ: {final_text[:120]}{'...' if len(final_text) > 120 else ''}")
        return Response(final_text, content_type="text/plain; charset=utf-8")

    except Exception as e:
        return Response(f"[error] {e}", content_type="text/plain; charset=utf-8")


if __name__ == "__main__":
    print("âœ… NLLB ç¿»è­¯ä¼ºæœå™¨å·²å•Ÿå‹• (port 5001)")
    app.run(host="0.0.0.0", port=5001)
